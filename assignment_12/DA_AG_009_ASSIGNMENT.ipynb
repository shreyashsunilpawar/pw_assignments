{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Question 1 — What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Definition:** Information Gain (IG) measures how much \"information\" a feature gives us about the class; more formally it is the reduction in entropy (uncertainty) about the target variable after splitting the dataset on a given attribute.\n",
        "\n",
        "**Mathematical formula:**\n",
        "- Entropy before split: \\(H(S) = -\\sum_{c} p(c)\\log_2 p(c)\\).\n",
        "- Entropy after splitting on attribute A with values v: \\(H(S|A) = \\sum_{v}\n",
        "rac{|S_v|}{|S|} H(S_v)\\).\n",
        "- Information Gain: \\(IG(S, A) = H(S) - H(S|A)\\).\n",
        "\n",
        "**How it's used in Decision Trees:**\n",
        "- During tree construction (e.g., ID3), at each node the algorithm evaluates candidate features and chooses the feature with the highest Information Gain to split on. This maximizes the reduction in uncertainty about the class.\n",
        "- The process repeats recursively on child nodes until stopping criteria (pure nodes, max depth, or minimum samples) are met.\n",
        "\n",
        "**Interpretation & properties:**\n",
        "- High IG means the feature produces child nodes that are more homogeneous (lower entropy) than the parent.\n",
        "- IG is biased toward features with many distinct values. To correct this, alternatives like *Gain Ratio* (which normalizes IG by intrinsic information) can be used.\n",
        "\n",
        "**Practical notes (for full marks):**\n",
        "- Computing IG requires estimation of class probabilities from available samples (so small sample sizes can give noisy estimates).\n",
        "- IG is a supervised measure (depends on labels). For continuous features, typical approach is to find the best threshold (binary split) that maximizes IG.\n",
        "- Example use in popular libraries: `scikit-learn`’s `DecisionTreeClassifier` supports `criterion='entropy'` to use entropy/IG-based splits."
      ],
      "metadata": {
        "id": "68RQTvZiSOTt"
      },
      "id": "68RQTvZiSOTt"
    },
    {
      "cell_type": "markdown",
      "id": "dc9cb2ff",
      "metadata": {
        "id": "dc9cb2ff"
      },
      "source": [
        "### Question 2 — What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Definitions:**\n",
        "- **Entropy (Information Gain criterion)**: \\(H(S) = -\\sum_{i} p_i \\log_2 p_i\\). It measures the average information needed to identify the class; sensitive to distribution tails.\n",
        "- **Gini Impurity**: \\(G(S) = 1 - \\sum_{i} p_i^2\\). It measures the probability of mislabeling a randomly chosen element if it were labeled according to the distribution.\n",
        "\n",
        "**Range and interpretation:**\n",
        "- Both range from 0 (pure node) to a maximum when the class distribution is uniform.\n",
        "- Gini is simpler to compute (no log) and often slightly faster.\n",
        "\n",
        "**Behavioral differences:**\n",
        "- **Sensitivity**: Entropy tends to be more sensitive to changes near the tails of the distribution; Gini tends to prefer larger partitions and is often slightly more biased toward splits that isolate the most frequent class.\n",
        "- **Split choice**: In practice, splits chosen by both metrics are often very similar. Differences do appear on some datasets, but rarely huge.\n",
        "\n",
        "**Strengths & weaknesses:**\n",
        "- **Entropy (IG)**:\n",
        "  - + Solid information-theoretic interpretation.\n",
        "  - − Slightly more computational cost due to log.\n",
        "  - − More sensitive to class probabilities, can prefer balanced splits that reduce entropy a lot.\n",
        "- **Gini**:\n",
        "  - + Computationally cheaper.\n",
        "  - + Often yields similar or slightly better practical performance in many cases.\n",
        "  - − Less interpretable from an information perspective.\n",
        "\n",
        "**When to use which:**\n",
        "- Use **Gini** when speed matters and interpretability of split choice via information theory is not required.\n",
        "- Use **Entropy/Information Gain** when you want the information-theoretic justification or when using algorithms (or assignments) that explicitly request entropy.\n",
        "\n",
        "**Summary line (for exam):** Gini is a measure of impurity based on squared probabilities and is computationally simpler; entropy is an information-theoretic impurity measure using logarithms. Both encourage purer child nodes; differences are small in practice but entropy has a stronger theoretical interpretation while Gini is slightly faster."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c18d86bb",
      "metadata": {
        "id": "c18d86bb"
      },
      "source": [
        "### Question 3 — What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Definition:** Pre-pruning (also called *early stopping*) is a technique to stop the growth of a decision tree early — i.e., while building the tree — by applying stopping criteria so that nodes are not split if they do not meet certain requirements.\n",
        "\n",
        "**Common pre-pruning criteria:**\n",
        "- Maximum tree depth (`max_depth`) — disallow splits deeper than this.\n",
        "- Minimum number of samples required to split (`min_samples_split`) or in a leaf (`min_samples_leaf`).\n",
        "- Minimum impurity decrease — only split if the impurity decrease is above a threshold.\n",
        "- Maximum number of leaf nodes.\n",
        "\n",
        "**Why use pre-pruning:**\n",
        "- Prevent **overfitting** by restricting tree complexity.\n",
        "- Reduce computational cost and improve generalization on unseen data.\n",
        "- Often necessary for noisy datasets or when the dataset is small.\n",
        "\n",
        "**Advantages:**\n",
        "- Simple to implement.\n",
        "- Controls model complexity during training.\n",
        "- Can dramatically improve generalization when tuned correctly.\n",
        "\n",
        "**Disadvantages:**\n",
        "- Might stop too early and underfit (miss important structure).\n",
        "- Choosing the correct thresholds requires validation (e.g., cross-validation).\n",
        "\n",
        "**Comparison with post-pruning:** Post-pruning grows a full tree and then prunes back (e.g., cost-complexity pruning). Post-pruning can potentially recover a better structure since it examines the full tree, but it requires more computation. Pre-pruning is faster but may prematurely cut useful branches."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2f4c977",
      "metadata": {
        "id": "b2f4c977"
      },
      "source": [
        "### Question 4 — Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances (practical).\n",
        "\n",
        "\n",
        "**Code:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee5d2e5b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee5d2e5b",
        "outputId": "451a99ed-037a-485a-8188-19c31b6ffe56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature importances (Iris, criterion='gini'):\n",
            "sepal length (cm): 0.0133\n",
            "sepal width (cm): 0.0000\n",
            "petal length (cm): 0.5641\n",
            "petal width (cm): 0.4226\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import numpy as np\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X, y)\n",
        "fi = clf.feature_importances_\n",
        "print(\"Feature importances (Iris, criterion='gini'):\")\n",
        "for name, val in zip(load_iris().feature_names, fi):\n",
        "    print(f\"{name}: {val:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72e44d27",
      "metadata": {
        "id": "72e44d27"
      },
      "source": [
        "### Question 5 — What is a Support Vector Machine (SVM)?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Definition:** SVM is a supervised learning algorithm for classification (and regression) that finds a decision boundary (hyperplane) which best separates classes by maximizing the margin — the distance between the hyperplane and the nearest data points of any class (the support vectors).\n",
        "\n",
        "**Key ideas:**\n",
        "- For a **linearly separable** dataset, SVM finds the hyperplane with maximum margin. This often yields good generalization.\n",
        "- SVM can handle non-separable data by allowing slack variables (soft margin) controlled by regularization parameter C.\n",
        "- For **nonlinear** boundaries, SVM uses the **kernel trick** to operate in a high-dimensional feature space without computing coordinates explicitly.\n",
        "\n",
        "**Mathematical form (binary):**\n",
        "\n",
        "The decision function of a binary SVM classifier is:\n",
        "\n",
        "\\[\n",
        "f(x) = \\text{sign}\\left( \\sum_{i=1}^{n} \\alpha_i \\, y_i \\, K(x_i, x) + b \\right)\n",
        "\\]\n",
        "\n",
        "where:\n",
        "\n",
        "- \\(\\alpha_i\\) are the Lagrange multipliers  \n",
        "- \\(y_i\\) are the class labels (+1 or -1)  \n",
        "- \\(x_i\\) are the support vectors  \n",
        "- \\(K(x_i, x)\\) is the kernel function  \n",
        "- \\(b\\) is the bias term  \n",
        "\n",
        "\n",
        "**Strengths:**\n",
        "- Effective in high-dimensional spaces.\n",
        "- Memory efficient — uses a subset of training points (support vectors).\n",
        "- Robust to overfitting when margin is large.\n",
        "\n",
        "**Weaknesses:**\n",
        "- Can be slow for very large datasets (training complexity grows with dataset size).\n",
        "- Choice of kernel and hyperparameters (C, gamma) is important.\n",
        "- Outputs are not probabilistic by default (though can be calibrated)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc1c6e68",
      "metadata": {
        "id": "dc1c6e68"
      },
      "source": [
        "### Question 6 — What is the Kernel Trick in SVM?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Definition:** The Kernel Trick refers to implicitly mapping input data into a higher-dimensional feature space using a kernel function \\(K(x, x')\\), which computes the inner product of the images of two points in that high-dimensional space without explicitly performing the mapping.\n",
        "\n",
        "**Why it's useful:**\n",
        "- Many datasets are not linearly separable in the original input space. Mapping to a higher-dimensional space can make them linearly separable.\n",
        "- Explicit mapping may be computationally expensive or infeasible (very high or infinite dimensional). Kernel functions let us compute dot products in that space efficiently.\n",
        "\n",
        "**Common kernels:**\n",
        "- **Linear:** \\(K(x, x') = x^\top x'\\)\n",
        "- **Polynomial:** \\(K(x, x') = (x^\top x' + c)^d\\)\n",
        "- **RBF (Gaussian):** \\(K(x, x') = \\exp(-\\gamma \\|x - x'\\|^2)\\) — very popular and powerful.\n",
        "\n",
        "**Practical implications:**\n",
        "- Using kernels lets SVM learn complex decision boundaries while solving the optimization problem in the original space using kernel evaluations.\n",
        "- Choice of kernel and hyperparameters (degree for poly, gamma for RBF) is crucial and typically done via cross-validation.\n",
        "\n",
        "**Intuition:** Kernel functions compute similarity in a transformed space; if two points are similar in that space, they influence each other's classification more strongly."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41742093",
      "metadata": {
        "id": "41742093"
      },
      "source": [
        "### Question 7 — Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.\n",
        "\n",
        "\n",
        "**Code (Python):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e23a7ce7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e23a7ce7",
        "outputId": "b214da56-d472-4bfb-8920-802b0f8941d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wine dataset accuracies (test set): Linear SVM: 0.9556, RBF SVM: 0.7111\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_wine(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
        "\n",
        "clf_lin = SVC(kernel='linear', random_state=42)\n",
        "clf_rbf = SVC(kernel='rbf', random_state=42)\n",
        "\n",
        "clf_lin.fit(X_train, y_train)\n",
        "clf_rbf.fit(X_train, y_train)\n",
        "\n",
        "acc_lin = accuracy_score(y_test, clf_lin.predict(X_test))\n",
        "acc_rbf = accuracy_score(y_test, clf_rbf.predict(X_test))\n",
        "print(f\"Wine dataset accuracies (test set): Linear SVM: {acc_lin:.4f}, RBF SVM: {acc_rbf:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b76fc45",
      "metadata": {
        "id": "7b76fc45"
      },
      "source": [
        "### Question 8 — What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Definition:** Naïve Bayes classifiers are a family of probabilistic classifiers based on Bayes' theorem:\n",
        "\\[\n",
        "P(y|x) =\n",
        "rac{P(y) \\prod_{i} P(x_i | y)}{P(x)}\n",
        "\\]\n",
        "where \\(x = (x_1, x_2, \\dots)\\).\n",
        "\n",
        "**\"Naïve\" assumption:** The classifier assumes that features are conditionally independent given the class label: \\(P(x|y) = \\prod_i P(x_i | y)\\). This simplification is usually false in practice (features are often correlated), hence the name *naïve*.\n",
        "\n",
        "**Why it still works:**\n",
        "- Despite the unrealistic independence assumption, Naïve Bayes often performs surprisingly well, especially for high-dimensional problems like text classification.\n",
        "- Fast to train, requires estimating relatively few parameters (class priors and per-feature likelihoods).\n",
        "\n",
        "**Usage:**\n",
        "- Variants include GaussianNB (continuous features), MultinomialNB (count data like word counts), and BernoulliNB (binary features).\n",
        "- Good baseline method and effective for problems where feature independence is approximately true or not critical."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d04f480a",
      "metadata": {
        "id": "d04f480a"
      },
      "source": [
        "### Question 9 —Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Gaussian Naïve Bayes (GaussianNB):**\n",
        "- **Assumption:** Each continuous feature follows a Gaussian (normal) distribution per class.\n",
        "- **Use case:** Continuous real-valued features (e.g., measurements). The likelihood \\(P(x_i|y)\\) is modeled using class-specific mean and variance.\n",
        "- **Example:** Sensor measurements, Iris dataset features.\n",
        "\n",
        "**Multinomial Naïve Bayes (MultinomialNB):**\n",
        "- **Assumption:** Feature vectors represent counts or frequencies (non-negative integers). Likelihood is modeled as a multinomial distribution.\n",
        "- **Use case:** Document classification with word counts or TF features.\n",
        "- **Example:** Bag-of-words models, where features are counts of words.\n",
        "\n",
        "**Bernoulli Naïve Bayes (BernoulliNB):**\n",
        "- **Assumption:** Features are binary (0/1) indicating presence/absence of a feature.\n",
        "- **Use case:** Binary occurrence features (e.g., whether a word appears in a document).\n",
        "- **Example:** Text classification with binary feature vectors indicating word presence.\n",
        "\n",
        "**Summary table:**\n",
        "- GaussianNB → continuous features (normal distribution).\n",
        "- MultinomialNB → count data (multinomial).\n",
        "- BernoulliNB → binary features (Bernoulli)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08495153",
      "metadata": {
        "id": "08495153"
      },
      "source": [
        "### Question 10 — Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy.\n",
        "\n",
        "\n",
        "**Code:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2feaf3f1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2feaf3f1",
        "outputId": "f3ae51c3-9a6c-48ed-9a1c-5856ff549496"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Breast Cancer dataset — GaussianNB accuracy (test set): 0.9371\n",
            "\n",
            "Classification report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9583    0.8679    0.9109        53\n",
            "           1     0.9263    0.9778    0.9514        90\n",
            "\n",
            "    accuracy                         0.9371       143\n",
            "   macro avg     0.9423    0.9229    0.9311       143\n",
            "weighted avg     0.9382    0.9371    0.9364       143\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
        "\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "y_pred = gnb.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Breast Cancer dataset — GaussianNB accuracy (test set): {acc:.4f}\")\n",
        "print(\"\\nClassification report:\\n\")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}