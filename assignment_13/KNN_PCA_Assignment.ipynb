{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "07ac7cce",
      "metadata": {
        "id": "07ac7cce"
      },
      "source": [
        "## Question 1\n",
        "### What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "K-Nearest Neighbors (KNN) is a supervised, instance-based learning algorithm that makes predictions based on the similarity between data points. It does not build an explicit model during training; instead, it stores the entire training dataset and performs computation only at prediction time.\n",
        "\n",
        "**Working Principle:**\n",
        "1. Choose the value of *k* (number of neighbors).\n",
        "2. Compute the distance between the test point and all training points (commonly Euclidean distance).\n",
        "3. Select the *k* closest neighbors.\n",
        "4. Aggregate their outputs to make a prediction.\n",
        "\n",
        "**Classification:**\n",
        "- The predicted class is the majority class among the *k* neighbors.\n",
        "- Example: If 3 out of 5 nearest neighbors belong to class A, the test sample is classified as class A.\n",
        "\n",
        "**Regression:**\n",
        "- The predicted value is the average (or weighted average) of the target values of the *k* neighbors.\n",
        "- Example: Predicting house prices using the mean price of nearby houses.\n",
        "\n",
        "**Advantages:** Simple, non-parametric, effective for small datasets.\n",
        "\n",
        "**Limitations:** Computationally expensive, sensitive to noise and feature scaling."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65682b82",
      "metadata": {
        "id": "65682b82"
      },
      "source": [
        "## Question 2\n",
        "### What is the Curse of Dimensionality and how does it affect KNN performance?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "The Curse of Dimensionality refers to problems that arise when working with high-dimensional data. As the number of features increases, the volume of the feature space grows exponentially, causing data points to become sparse.\n",
        "\n",
        "**Effect on KNN:**\n",
        "- Distances between points become less meaningful.\n",
        "- The difference between nearest and farthest neighbors shrinks.\n",
        "- KNN struggles to find truly \"close\" neighbors.\n",
        "\n",
        "**Consequences:**\n",
        "- Reduced classification accuracy.\n",
        "- Increased computational cost.\n",
        "- Higher risk of overfitting.\n",
        "\n",
        "**Mitigation:** Dimensionality reduction techniques such as PCA and proper feature scaling."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6e03658",
      "metadata": {
        "id": "a6e03658"
      },
      "source": [
        "## Question 3\n",
        "### What is Principal Component Analysis (PCA)? How is it different from feature selection?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Principal Component Analysis (PCA) is an unsupervised dimensionality reduction technique used in data analysis and machine learning to reduce the number of features in a dataset while preserving as much important information (variance) as possible. PCA achieves this by transforming the original correlated features into a new set of uncorrelated variables called principal components.\n",
        "\n",
        "**Feature Selection Explained:**\n",
        "Feature selection involves choosing the most relevant features from the original dataset without transforming them. Common techniques include:\n",
        "- Filter methods (correlation, chi-square).\n",
        "- Wrapper methods (recursive feature elimination).\n",
        "- Embedded methods (Lasso, tree-based importance).\n",
        "\n",
        "**Key Differences:**\n",
        "- PCA creates new features, while feature selection chooses existing ones.\n",
        "- PCA focuses on maximizing variance, whereas feature selection focuses on feature relevance.\n",
        "- PCA may reduce interpretability, but feature selection keeps features understandable."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "debf84ac",
      "metadata": {
        "id": "debf84ac"
      },
      "source": [
        "## Question 4\n",
        "### What are eigenvalues and eigenvectors in PCA, and why are they important?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "In Principal Component Analysis (PCA), eigenvalues and eigenvectors are fundamental mathematical concepts used to identify the most important directions in which the data varies.\n",
        "\n",
        "PCA is based on the covariance matrix of the dataset, which captures how features vary with respect to each other. Eigenvalues and eigenvectors are computed from this covariance matrix.\n",
        "\n",
        "**Importance:**\n",
        "1) Dimensionality Reduction - Eigenvalues allow us to select only those components that explain most of the variance, reducing dimensionality while preserving information.\n",
        "2) Noise Reduction- Components with very small eigenvalues often represent noise and can be discarded.\n",
        "3) Explained Variance -Eigenvalues are used to compute the explained variance ratio, which helps in selecting components (e.g., 95% variance rule).\n",
        "4) Efficient Feature Transformation-\n",
        "Eigenvectors define the transformation from original features to principal components, ensuring minimal information loss."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42b1910e",
      "metadata": {
        "id": "42b1910e"
      },
      "source": [
        "## Question 5\n",
        "### How do KNN and PCA complement each other when applied in a single pipeline?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "- PCA is a dimensionality reduction technique that transforms high-dimensional data into fewer uncorrelated principal components.\n",
        "\n",
        "- KNN is a distance-based algorithm whose performance depends on meaningful distance calculations.\n",
        "\n",
        "- In high-dimensional data, KNN suffers from the curse of dimensionality, where distances become less reliable.\n",
        "\n",
        "- PCA reduces dimensions, removes noise, and eliminates correlated features.\n",
        "\n",
        "- After PCA, distances between data points are more meaningful, improving KNN accuracy.\n",
        "\n",
        "- PCA reduces computational cost and overfitting.\n",
        "\n",
        "- The combined PCA + KNN pipeline improves efficiency, generalization, and model performance.\n",
        "\n",
        "- This pipeline is widely used in image processing, gene expression analysis, and text data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "680ea74d",
      "metadata": {
        "id": "680ea74d"
      },
      "source": [
        "## Question 6\n",
        "### Train a KNN Classifier on the Wine dataset with and without feature scaling. Compare model accuracy in both cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5b88638b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b88638b",
        "outputId": "46d306ef-6d3f-4ad4-f03e-88757170500f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7222222222222222, 0.9444444444444444)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_wine(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Without scaling\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "acc_no_scaling = accuracy_score(y_test, knn.predict(X_test))\n",
        "\n",
        "# With scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "acc_scaling = accuracy_score(y_test, knn.predict(X_test_scaled))\n",
        "\n",
        "acc_no_scaling, acc_scaling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "428d8b0e",
      "metadata": {
        "id": "428d8b0e"
      },
      "source": [
        "## Question 7\n",
        "### Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b240125a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b240125a",
        "outputId": "a7b097e7-02c3-4187-f3b5-dcf8f568dba0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.35900066, 0.18691934, 0.11606557, 0.07371716, 0.0665386 ,\n",
              "       0.04854582, 0.04195042, 0.02683922, 0.0234746 , 0.01889734,\n",
              "       0.01715943, 0.01262928, 0.00826257])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA()\n",
        "pca.fit(X_train_scaled)\n",
        "pca.explained_variance_ratio_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fdbe86c",
      "metadata": {
        "id": "8fdbe86c"
      },
      "source": [
        "## Question 8\n",
        "### Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components). Compare the accuracy with the original dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5a34b936",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a34b936",
        "outputId": "5f9af6e2-35c1-4438-b147-fea4623e4bfd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "\n",
        "pca2 = PCA(n_components=2)\n",
        "X_train_pca = pca2.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca2.transform(X_test_scaled)\n",
        "\n",
        "knn.fit(X_train_pca, y_train)\n",
        "acc_pca = accuracy_score(y_test, knn.predict(X_test_pca))\n",
        "\n",
        "acc_pca\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62406d30",
      "metadata": {
        "id": "62406d30"
      },
      "source": [
        "## Question 9\n",
        "### Train a KNN Classifier with different distance metrics (euclidean,manhattan) on the scaled Wine dataset and compare the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5e1c3af7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e1c3af7",
        "outputId": "6cfc9016-1596-41f1-ab68-7c82a4cfc79a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'euclidean': 0.9444444444444444, 'manhattan': 0.9444444444444444}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "\n",
        "results = {}\n",
        "for metric in ['euclidean', 'manhattan']:\n",
        "    knn = KNeighborsClassifier(n_neighbors=5, metric=metric)\n",
        "    knn.fit(X_train_scaled, y_train)\n",
        "    results[metric] = accuracy_score(y_test, knn.predict(X_test_scaled))\n",
        "\n",
        "results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "611533e7",
      "metadata": {
        "id": "611533e7"
      },
      "source": [
        "## Question 10\n",
        "### You are working with a high-dimensional gene expression dataset to\n",
        "classify patients with different types of cancer.\n",
        "Due to the large number of features and a small number of samples, traditional models\n",
        "overfit.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "1. **Using PCA to Reduce Dimensionality:**\n",
        "- Apply PCA to transform gene features into principal components\n",
        "- Retains maximum variance while removing noise and redundancy\n",
        "- Reduces risk of overfitting\n",
        "2. **Deciding Number of Components:**\n",
        "- Use explained variance ratio (90â€“95%)\n",
        "- Analyze cumulative variance / elbow plot\n",
        "- Validate using cross-validation performance\n",
        "3. **Using KNN After PCA:**\n",
        "- Train KNN on PCA-transformed data\n",
        "- PCA improves distance calculations\n",
        "- Tune value of k using validation\n",
        "4. **Model Evaluation:**\n",
        "- Cross-validation for reliability\n",
        "- Metrics:\n",
        "    - Accuracy\n",
        "    - Precision & Recall (important in cancer diagnosis)\n",
        "    - F1-score\n",
        "    - Confusion Matrix\n",
        "    - ROC-AUC\n",
        "\n",
        "5. **Justification to Stakeholders:**\n",
        "- Reduces overfitting\n",
        "- Computationally efficient\n",
        "- Robust for small datasets\n",
        "- Clinically reliable and scalable\n",
        "- Suitable for real-world biomedical applications"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}